<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="google-site-verification" content="CLAZwNRfx6oIwU-FwsUMBEOhwd64lMXSyfkvuubm_kg" />
    <title>Data Piques | articles in the "misc" category</title>
    <link rel="shortcut icon" type="image/png" href="http://blog.ethanrosenthal.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://blog.ethanrosenthal.com/favicon.ico">
    <link href="http://blog.ethanrosenthal.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Data Piques Full Atom Feed" />
    <link href="http://blog.ethanrosenthal.com/feeds/misc.atom.xml" type="application/atom+xml" rel="alternate" title="Data Piques Categories Atom Feed" />
    <link rel="stylesheet" href="http://blog.ethanrosenthal.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://blog.ethanrosenthal.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://blog.ethanrosenthal.com/theme/css/print.css" type="text/css" media="print" />

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <meta name="generator" content="Pelican" />
</head>
<body>
    <header>
        <nav>
            <ul>

                <li class="ephemeral selected"><a href="http://blog.ethanrosenthal.com/category/index.html">misc</a></li>
                <li><a href="http://blog.ethanrosenthal.com">Home</a></li>
                <li><a href="/archives.html">Archives</a></li>
                <li><a href="/pages/about.html">About</a></li>
                <li><a href="http://www.ethanrosenthal.com">Website</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://blog.ethanrosenthal.com">Data Piques</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Jun 20, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2017/06/20/matrix-factorization-in-pytorch/" rel="bookmark" title="Permanent Link to &quot;Matrix Factorization in PyTorch&quot;">Matrix Factorization in PyTorch</a>
                </h2>

                
<p>Hey, remember when I wrote those <a href="http://blog.ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/">ungodly</a> <a href="http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/">long</a> <a href="http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/">posts</a> about matrix factorization chock-full of gory math? Good news! You can forget it all. We have now entered the Era of Deep Learning, and automatic differentiation shall be our guiding light.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Mar 20, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2017/03/20/analytical-numerical-universal/" rel="bookmark" title="Permanent Link to &quot;From Analytical to Numerical to Universal Solutions&quot;">From Analytical to Numerical to Universal Solutions</a>
                </h2>

                <p>I've been making my way through the recently released <a href="http://www.deeplearningbook.org/">Deep Learning</a> textbook (which is absolutely excellent), and I came upon the section on Universal Approximation Properties. The <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a> (UAT) essentially proves that neural networks are capable of approximating any continuous function (subject to some constraints and with upper â€¦</p>
                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Feb 05, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2017/02/05/rec-a-sketch/" rel="bookmark" title="Permanent Link to &quot;Rec-a-Sketch: a Flask App for Interactive Sketchfab Recommendations&quot;">Rec-a-Sketch: a Flask App for Interactive Sketchfab Recommendations</a>
                </h2>

                
<p>After the long <a href="http://blog.ethanrosenthal.com/2016/10/09/likes-out-guerilla-dataset/">series</a> <a href="http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/">of</a> <a href="http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/">previous</a> <a href="http://blog.ethanrosenthal.com/2016/12/05/recasketch-keras/">posts</a> describing various recommendation algorithms using Sketchfab data, I decided to build a website called <a href="http://www.rec-a-sketch.science/">Rec-a-Sketch</a> which visualizes the different algorithms' recommendations. In this post, I'll describe the process of getting this website up and running on AWS with nginx and gunicorn.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Dec 05, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/12/05/recasketch-keras/" rel="bookmark" title="Permanent Link to &quot;Using Keras' Pretrained Neural Networks for Visual Similarity Recommendations&quot;">Using Keras' Pretrained Neural Networks for Visual Similarity Recommendations</a>
                </h2>

                
<p>To close out our series on building recommendation models using <a href="http://blog.ethanrosenthal.com/2016/10/09/likes-out-guerilla-dataset/">Sketchfab data</a>, I will venture far from the <a href="http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/">previous</a> <a href="http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/">posts'</a> factorization-based methods and instead explore an unsupervised, deep learning-based model. You'll find that the implementation is fairly simple with remarkably promising results which is almost a smack in the face to all of that effort put in earlier.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Nov 07, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/11/07/implicit-mf-part-2/" rel="bookmark" title="Permanent Link to &quot;Learning to Rank Sketchfab Models with LightFM&quot;">Learning to Rank Sketchfab Models with LightFM</a>
                </h2>

                
<p>In this post we're going to do a bunch of cool things following up on the last <a href="http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/">post</a> introducing implicit matrix factorization. We're going to explore Learning to Rank, a different method for implicit matrix factorization, and then use the library <a href="http://lyst.github.io/lightfm/docs/home.html">LightFM</a> to incorporate side information into our recommender. Next, we'll use <a href="https://scikit-optimize.github.io/">scikit-optimize</a> to be smarter than grid search for cross validating hyperparameters. Lastly, we'll see that we can move beyond simple user-to-item and item-to-item recommendations now that we have side information embedded in the same space as our users and items. Let's go!</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Oct 19, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/" rel="bookmark" title="Permanent Link to &quot;Intro to Implicit Matrix Factorization: Classic ALS with Sketchfab Models&quot;">Intro to Implicit Matrix Factorization: Classic ALS with Sketchfab Models</a>
                </h2>

                
<p>Last post I described how I collected implicit feedback data from the website <a href="https://sketchfab.com/">Sketchfab</a>. I then claimed I would write about how to actually build a recommendation system with this data. Well, here we are! Let's build.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Oct 09, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/10/09/likes-out-guerilla-dataset/" rel="bookmark" title="Permanent Link to &quot;Likes Out! Guerilla Dataset!&quot;">Likes Out! Guerilla Dataset!</a>
                </h2>

                
<p>-- <a href="https://www.youtube.com/watch?v=H0kJLW2EwMg">Zack de la Rocha</a></p>
<p><em>tl;dr -> I collected an implicit feedback dataset along with side-information about the items. This dataset contains around 62,000 users and 28,000 items. All the data lives <a href="https://github.com/EthanRosenthal/rec-a-sketch/tree/master/data">here</a> inside of <a href="https://github.com/EthanRosenthal/rec-a-sketch">this</a> repo. Enjoy!</em></p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Aug 30, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/08/30/towards-optimal-personalization/" rel="bookmark" title="Permanent Link to &quot;Towards optimal personalization: synthesisizing machine learning and operations research&quot;">Towards optimal personalization: synthesisizing machine learning and operations research</a>
                </h2>

                
<p>Last <a href="http://blog.ethanrosenthal.com/2016/07/20/lets-talk-or/">post</a> I talked about how data scientists probably ought to spend some time talking about optimization (but not too much time - I need topics for my blog posts!). While I provided a basic optimization example in that post, that may have not been so interesting, and there definitely wasn't any machine learning involved.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Jul 20, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/07/20/lets-talk-or/" rel="bookmark" title="Permanent Link to &quot;I'm all about ML, but let's talk about OR&quot;">I'm all about ML, but let's talk about OR</a>
                </h2>

                
<p>You've studied machine learning, you're a dataframe master for massaging data, and you can easily pipe that data through a bunch of machine learning libraries.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>            <h4 class="date">Jan 09, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://blog.ethanrosenthal.com/2016/01/09/explicit-matrix-factorization-sgd-als/" rel="bookmark" title="Permanent Link to &quot;Explicit Matrix Factorization: ALS, SGD, and All That Jazz&quot;">Explicit Matrix Factorization: ALS, SGD, and All That Jazz</a>
                </h2>

                
<p>In my last <a href="http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/">post</a>, I described user- and item-based collaborative filtering which are some of the simplest recommendation algorithms. For someone who is used to conventional machine learning classification and regression algorithms, collaborative filtering may have felt a bit <em>off</em>. To me, machine learning almost always deals with some function which we are trying to maximize or minimize. In simple linear regression, we minimize the mean squared distance between our predictions and the true values. Logistic regression involves maximizing a likelihood function. However, in my post on collaborative filtering, we randomly tried a bunch of different parameters (distance function, top-k cutoff) and watched what happened to the mean squared error. This sure doesn't feel like machine <em>learning</em>.</p>

                <div class="clear"></div>

                <div class="info">
                </div>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="http://blog.ethanrosenthal.com/page/2" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 1 of 2</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="http://blog.ethanrosenthal.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-54232305-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>